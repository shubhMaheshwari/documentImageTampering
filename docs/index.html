<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Document Tampering Detection</title>

  <!-- CSS  -->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
</head>
<body>
  <nav class="white" role="navigation">
	<div class="nav-wrapper container">

	  <ul class="center hide-on-med-and-down">
		<li><a href="#intorduction">Intorduction</a></li>
		<li><a href="#Dataset">Dataset</a></li>
		<li><a href="#experiment">Experiment</a></li>
		<li><a href="#results">Results</a></li>
		<li><a href="#improvements">Improvements</a></li>
		<li><a href="#conclusion">Conclusion</a></li>
	  </ul>

	  <ul id="nav-mobile" class="sidenav">
		<li><a href="#intorduction">Intorduction</a></li>
		<li><a href="#Dataset">Dataset</a></li>
		<li><a href="#experiment">Experiment</a></li>
		<li><a href="#results">Results</a></li>
		<li><a href="#improvements">Improvements</a></li>
		<li><a href="#conclusion">Conclusion</a></li>
	  </ul>
	  <a href="#" data-target="nav-mobile" class="sidenav-trigger"><i class="material-icons">menu</i></a>
	</div>
  </nav>

  <!-- <div id="index-banner" class="parallax-container"> -->
	<div class="section no-pad-bot">
	  <div class="container">
		<br><br>
		<h1 class="header center teal-text text-lighten-2">Document Tampering Detection</h1>

	  </div>
	</div>
  <!-- </div> -->

 <div class="container" id="lastest">
	<div class="section">
	  <!--   Icon Section   -->
	  <div class="row center" style="font-size: 19px">
			<a class="waves-effect waves-light btn" href="./Report.pdf">Previous Semester Report</a>
			<a class="waves-effect waves-light btn" href="#current_experiment">Current Plan</a>
			<a class="waves-effect waves-light btn" href="http://preon.iiit.ac.in:9090/">Demo Website</a>

	  </div>
	</div>
  </div>



  <div class="container" id="introduction">
	<div class="section">

	  <!--   Icon Section   -->
	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Introduction</h3>
			<p class=" light" style="margin: 0em"> Document Tampering detection is the find the location of tampering in a document. Convolution networks provide good results on detection of image tampering but these network require large amount of training data. Thus, we want to show that using data augmentation and domain adaptation we can get much better results. </p>
	  </div>
	</div>
  </div>

  <div class="container" id="dataset">

	  <!--   Icon Section   -->
	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Findit Dataset</h3>
		  <p class="center light" style="margin: 0em" >For detection we are using the <a href="http://dx.doi.org/10.1109/ICPR.2018.8545428">Find-it dataset [4]</a>. The dataset is divided for 2 tasks classification and detection. <br> 
		  For detection it provides 100 images for training and 80 images for testing.</p>   
		  <div class="row">
			  <ul class="collapsible">
				<li>
				  <div class="collapsible-header">Images from dataset for Task 1. Classification</div>
				  <div class="collapsible-body">
					<p>For classification the datasets provides 500(470 pristine and 30 tampered) images for testing and 499(469 pristine and 30 tampered) images for testing. </p>
					<div class="row">
					  <div class="col s6">
						<p>Tampered images</p>
						<br/>
						<img style="width: 45%;" src="./images/tampered1.jpg" > 
						<img style="width: 45%;" src="./images/tampered2.jpg" > 
					  </div>
					  <div class="col s6">
						<p>Non tampered images</p>
						<img style="width: 45%" src="./images/non_tampered1.jpg" > 
						<img style="width: 45%" src="./images/non_tampered2.jpg" > 
					  </div>
					</div>
				  </div>
				</li>
				<li>
				  <div class="collapsible-header">Images from dataset for Task 2. Detection</div>
				  <div style="width: 100%;align-items: flex-start" class="collapsible-body">
				  	<p>For detection it provides 100 images for training and 80 images for testing.</p>
					<img style="width: 45%" src="./images/tampered_loc1.png" > 
					<img style="width: 45%" src="./images/tampered_loc3.png" > 
				  </div>
				</li>
			  </ul>

		  </div>
		  
		  <div class="row">
			<p class="center light" style="margin: 0em"> Table below shows the Distribuition of types of tampering in the dataset</p>
			<div class="col s6">
				<img style="width: 100%;" src="./images/datatypes.png">
			</div>
			<div class="col s6">
				<br>
				<br>
				<ul>
				  <li > CPI (copy and paste inside the document) </li>
				  <li >CPO (copy and paste from an other document)</li>
				  <li >IMI (creation of a text box imitating the font)</li>
				  <li >CUT (deletion of one or more characters/words)</li>
				  <li >Other: drawing, copy and paste from web...</li>
				</ul>
			</div>
		  </div>  

		  <div class="row">
			<h3 class="center brown-text" style="margin: 0em">Synthetic Data creation</h3>
			<p>As tampered images are so low.  It makes it very hard to train a CNN architecture with such data.  Hence we create synthetic tampered images using 3 types of tampering.</p>
			<ul>
			  <li >1. Copy-paste or CPI </li>
			  <li >2. Splicing or CPO </li>
			  <li >3. In-painting or CUT </li>
			</ul>

		  <ul class="collapsible">
			<li>
			  <div class="collapsible-header">Examples of augmented images created. </div>
			  <div style="width: 100%;align-items: flex-start" class="collapsible-body">
			  	<div class="row">
						<p class="center">Copy-Paste</p>
						<img style="width: 110%" src="./images/report_images/Figure_1-1.png" > 

						<p class="center">Splicing</p>
						<img style="width: 100%" src="./images/report_images/Figure_1-3.png" > 
					  
						<p class="center">Inpainting</p>
						<img style="width: 100%" src="./images/report_images/Figure_1-2.png" > 
				</div>
			  	</div>
				</li>
		  	</ul>
		  	<p> Find-it dataset provides 470 pristine images for classification. To create tampered images we first use a <a href="http://arxiv.org/abs/1609.03605">ctpn-textdetector [3]</a> to extract text from the documents.  To extract characters from the text, we use connectedcomponents.  Finally different types of tampering are created by replacing the text region.Thus, we create total of 6000 synthetic images. </p>

		  </div>  
	  </div>
  </div>


  <div class="container" id="experiment">
	<div class="section">

	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Method</h3>

		<h5 class="center brown-text">Creating Patches</h5>
		<p>
			The input to the model are 64x64 patches. To extract patches from the image. First, we run a text detector on the documents to detect the text. We use the <a href="http://arxiv.org/abs/1609.03605">ctpn-textdetector [3]</a>. Then 64x64 patches are created by extracting the region around the text. This becomes the input to our model
		</p>

		<h5 class="center brown-text">Class Imbalance</h5>
		<p>
			As a document generally contains large amount of text out of which only a few regions are tampered. The data becomes highly imbalanced. In the find-it dataset we noticed that on average only 3.0% of patches extracted from a document were tampered. Hence during training tampered patches are over-sampled. We kept the ratio at ratio of 32:1. The chance of picking a tampered patch is 32 times more likely than a pristine patch.
		</p>

		<h5 class="center brown-text">Architecture</h5>
		<p>
			<img class="center" style="width: 70% " src="./images/report_images/architecture.png" > 
			As seen in the figure during training source model and target model are trained simultaneously. Yashas uses a 5 layer CNN as the trunk of our architecture. The weights between them are shared.  Next they are passed through 2 fully connected layers, to get a 256-dim representation for source and target images. For domain adapation Yashas defines an MMD loss between the 2 representations. The MMD loss defined as:
		</p>
		<img class="center" style="width: 70%; padding-left: 30%" src="./images/report_images/mmd_loss.png" > 
		<p>where xs and xt are features for the source and target images respectively. Φ(xs) and Φ(xt) are the calculated by passing the representation through Gaussian kernel</p>

		<h5 class="center brown-text">Experiments</h5>		
	  	<div class="row">
		  <ul class="collapsible">
			<li>
			  <div class="collapsible-header">Training & Validation loss with and without using augmented data</div>
			  <div class="collapsible-body">
				<div class="row">
					<img style="width: 100%" src="./images/report_images/loss_plot.png" > 
				</div>
			  </div>
			</li>
			<li>
			  <div class="collapsible-header"> Training and Validation accuracy with and without using augmented data</div>
			  <div style="width: 100%;align-items: flex-start" class="collapsible-body">
			  	<p>Validation accuracy on the find-it images is shown by pink color.</p>
				<img style="width: 100%" src="./images/report_images/acc_plot.png" > 
			  </div>
			</li>
			<li>
			  <div class="collapsible-header"> Training and Validation F1-scores with and without using augmented data</div>
			  <div style="width: 100%;align-items: flex-start" class="collapsible-body">
			  	<p>As accuracy is not a good metric for evaluating hence we are also measuring the f1 score during training. Figure shows the training and validation f1-scores with and without using augmented data. We can clearly see that the model has over-fitted on the training data and hence is performing poorly on validation.</p>
				<img style="width: 100%" src="./images/report_images/f1-score.png" > 
			  </div>
			</li>

		  </ul>
		</div>

		<h5 class="center black-test" > Testing </h5>
		<p>
			For testing we have taken the model with the highest average IOU over the validation data.
		</p>
		<table>
	        <thead>
	          <tr>
	              <th>Average IOU on test data</th>
	          </tr>
	        </thead>
	        <tbody>
	          <tr>
	            <td>Without Augmentation</td>
	            <td>0.00138</td>
	          </tr>
	          <tr>
	            <td>With Augmentation</td>
	            <td>0.00178</td>
	          </tr>
	        </tbody>
	    </table>

		<h5 class="center black-test" > Examples </h5>
		<ul class="collapsible">
			<li>
			  <div class="collapsible-header">Qualtitative Results on Test set</div>
			  <div class="row collapsible-body">
					<img style="width: 100%" src="./images/report_images/testbetter1.png" > 
					<img style="width: 100%" src="./images/report_images/testbetter2.png" > 
			  </div>
			</li>
		</ul>
		<p> For more results go to this <a>link</a> </p>

		<h3 class="center brown-text">Conclusion</h3>
		<ul>
			<li >1. Even after using augmented data the results were very poor on the find-it dataset</li>
			<li >2. We tried undersampling pristine patches but the model still seems to be overfitting on the training data.</li>
		</ul>
		 
	  </div>
	</div>
  </div>  


  <div class="container" id="current_experiment">
	<div class="section">

	  <!--   Icon Section   -->
	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Current Plan</h3>
			
			<ul>
				<li >1. We are planning to change the objective from patch classification to semantic segmentation. </li>
				<li> 2. Also we are planning to use noiseprint features</li>
			</ul>


			<img style="width: 100%" src="./images/unet.png" > 
			<p><b>Clues why semantic segmentation would give better</b></p>
			<ul>
				<li>
					1. This should help in finding splicing artifacts as the spliced patch should have different PRNU features than the rest of the image.  
				</li>
				<li>
					2. This might help in copy-move based forgery if the model is able to find similiar patches. 
				</li>
				<li>	
					3. This should help in inpainting if the model is able to understand the pattern generated using inpainting.
				</li>
			</ul>

		<!-- Noise print -->            
		<div class="row">
		  <b class="center black-text">Noiseprint Features<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8713484">[2]</a></b>
		  <p class="center light" style="margin: 0em"> The tampered region has noise inconsistencies in the tampered region. A CNN based network is used to extract noise from images whose architecture is shown below.
		  </p>
		  <img style="display: block;margin-left: auto;margin-right: auto;width: 50%;" src="./images/noiseprint.png" > <p class="center">The network architecture</p>
		  <img style="display: block;margin-left: auto;margin-right: auto;width: 50%;" src="./images/noiseprint-example.png" > <p class="center">Example of noise inconsistencies</p>
		  This network is used to detect forgeries in an image, as tampered regions will have inconsistencies in noise as shown below.
		</div>
	
	  </div>
	</div>
  </div>


<!--   <div class="container" id="improvements">
	<div class="section"> -->

	  <!--   Icon Section   -->
<!-- 	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Results on classification</h3>
		<p> </p>
	  </div> 
	</div>
  </div>
  </div>
 -->

  <footer class="page-footer teal">
	<div class="container">
	  <div class="row">
		<div class="col l6 s12">
		  <h5 class="white-text">Related Works</h5>
		  <ul>
			<li><a class="white-text" href="https://arxiv.org/pdf/1811.09729v3.pdf">[1] Generate, Segment and Refine: Towards Generic Manipulation Segmentation</a></li>
			<li><a class="white-text" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8713484">[2] Noiseprint: A CNN-Based Camera Model Fingerprint</a></li>
			<li><a class="white-text" href="https://minyoungg.github.io/selfconsistency/">[3] Z. Tian, W. Huang, T. He, P. He, Y. Qiao, Detecting text in natural image with connectionist text proposal network (2016)</a></li>
			<li><a class="white-text" href="http://dx.doi.org/10.1109/ICPR.2018.8545428">[4] C. Artaud, N. Sidre, A. Doucet, J. Ogier, V. P. D. Yooz, Find it! fraud detection contest report, in: 2018 24th InternationalConference on Pattern Recognition (ICPR)</a></li>
		  </ul>
		</div>
		<div class="col l6 s12">
		  <ul>
		  	<br>
		  	<br>
			<li><a class="white-text" href="https://arxiv.org/pdf/1811.09729v3.pdf">[5] Generate, Segment and Refine: Towards Generic Manipulation Segmentation</a></li>
			<li><a class="white-text" href="https://arxiv.org/pdf/1906.11663.pdf">[6] SpliceRadar: A Learned Method For Blind Image Forensics</a></li>
			<li><a class="white-text" href="https://minyoungg.github.io/selfconsistency/">[7] Fighting Fake News: Image Splice Detection via Learned Self-Consistency</a></li>
		  </ul>
		</div>
		</div>
	</div>
	<div class="footer-copyright">
	  <div class="container">
	  Made by <a class="brown-text text-lighten-3" href="#">Shubh </a>
	  </div>
	</div>
  </footer>


  <!--  Scripts-->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="js/materialize.js"></script>
  <script src="js/init.js"></script>

  </body>
</html>
