<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Document Tampering Detection</title>

  <!-- CSS  -->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
</head>
<body>
  <nav class="white" role="navigation">
	<div class="nav-wrapper container">

	  <ul class="center hide-on-med-and-down">
		<li><a href="#intorduction">Intorduction</a></li>
		<li><a href="#dataset">Dataset</a></li>
		<li><a href="#method">Method</a></li>
		<li><a href="#experiment">Experiment</a></li>
	  </ul>

	  <ul id="nav-mobile" class="sidenav">
		<li><a href="#intorduction">Intorduction</a></li>
		<li><a href="#dataset">Dataset</a></li>
		<li><a href="#method">Method</a></li>
		<li><a href="#experiment">Experiment</a></li>
	  </ul>
	  <a href="#" data-target="nav-mobile" class="sidenav-trigger"><i class="material-icons">menu</i></a>
	</div>
  </nav>

  <!-- <div id="index-banner" class="parallax-container"> -->
	<div class="section no-pad-bot">
	  <div class="container">
		<br><br>
		<h1 class="header center teal-text text-lighten-2">Document Tampering Detection</h1>

	  </div>
	</div>
  <!-- </div> -->

 <div class="container" id="lastest">
	<div class="section">
	  <!--   Icon Section   -->
	  <div class="row center" style="font-size: 19px">
<!-- 	  		<a class="waves-effect waves-light btn" href="#current_experiment">Current Plan</a> -->
			<a class="waves-effect waves-light btn" href="./Report.pdf">Previous Semester Report</a>
			<a class="waves-effect waves-light btn" href="http://preon.iiit.ac.in:9090/">Demo Website</a>
			<a class="waves-effect waves-light btn" href="https://shubhmaheshwari.github.io/Image_Tampering/">Image Tampering</a>
	  </div>
	</div>
  </div>



  <div class="container" id="introduction">
	<div class="section">

	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Current Findings</h3>
		<table>
	        <thead>
	        </thead>
	        <tbody>
	          <tr>
	            <td>1. With only 274 patches to train a deep learning setup is unable to learn features for document tampering detection. I have tried multiple setups such as different sampling rate, different architeures, different patch sizes. See <a href="#experiment">Experiments Section (cite)</a> </td>
	          </tr>
	          <tr>
	           	<td>2. Data augmention and domain adaptation similiar to Yashas Setup for Image tampering increases the performance of the model but it is not sufficient to be used for detection. </td>
	           </tr>
	           <tr>
	            <td>3. Combination of non deep learning methods (<a href="https://shubhmaheshwari.github.io/Image_Tampering/#CMFD_method" >CMFD, Jpeg Artifacts and Splicebuster </a>) seems to be performing better than the current setup.  </td>
	        	</tr>
	        	<tr>
	          	<td>4. A better statergy to create synthethic document tampering should boost performance.  </td>
	          </tr>
	        </tbody>
	    </table>
	  </div>


	  <!--   Icon Section   -->
	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Introduction</h3>
			<p class=" light" style="margin: 0em"> Document Tampering detection is the find the location of tampering in a document. Convolution networks provide good results on detection of image tampering but these network require large amount of training data. Thus, we want to see if using data augmentation and domain adaptation we can get better performance using previous approaches. </p>
	  </div>
	</div>
  </div>

  <div class="container" id="dataset">

	  <!--   Icon Section   -->
	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Findit Dataset</h3>
		  <p class="center light" style="margin: 0em" >For detection we are using the <a href="http://dx.doi.org/10.1109/ICPR.2018.8545428">Find-it dataset [4]</a>. The dataset is divided for 2 tasks classification and detection. <br> 
		  For detection it provides 100 images for training and 80 images for testing.</p>   
		  <div class="row">
			  <ul class="collapsible">
				<li>
				  <div class="collapsible-header">Images from dataset for Task 1. Classification</div>
				  <div class="collapsible-body">
					<p>For classification the datasets provides 500(470 pristine and 30 tampered) images for testing and 499(469 pristine and 30 tampered) images for testing. </p>
					<div class="row">
					  <div class="col s6">
						<p>Tampered images</p>
						<br/>
						<img style="width: 45%;" src="./images/tampered1.jpg" > 
						<img style="width: 45%;" src="./images/tampered2.jpg" > 
					  </div>
					  <div class="col s6">
						<p>Non tampered images</p>
						<img style="width: 45%" src="./images/non_tampered1.jpg" > 
						<img style="width: 45%" src="./images/non_tampered2.jpg" > 
					  </div>
					</div>
				  </div>
				</li>
				<li>
				  <div class="collapsible-header">Images from dataset for Task 2. Detection</div>
				  <div style="width: 100%;align-items: flex-start" class="collapsible-body">
				  	<p>For detection it provides 100 images for training and 80 images for testing.</p>
					<img style="width: 45%" src="./images/tampered_loc1.png" > 
					<img style="width: 45%" src="./images/tampered_loc3.png" > 
				  </div>
				</li>
			  </ul>

		  </div>
		  
		  <div class="row">
			<p class="center light" style="margin: 0em"> Table below shows the Distribuition of types of tampering in the dataset</p>
			<div class="col s6">
				<img style="width: 100%;" src="./images/datatypes.png">
			</div>
			<div class="col s6">
				<br>
				<br>
				<ul>
				  <li > CPI (copy and paste inside the document) </li>
				  <li >CPO (copy and paste from an other document)</li>
				  <li >IMI (creation of a text box imitating the font)</li>
				  <li >CUT (deletion of one or more characters/words)</li>
				  <li >Other: drawing, copy and paste from web...</li>
				</ul>
			</div>
		  </div>    
	  </div>
  </div>



  

  <div class="container" id="method">
	<div class="section">

	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Method</h3>


		<div class="row">
	  	<img src="./images/Document_Tampering_setup.png" style="width: 100% "/>
			<h6 class="center black-text">Figure shows the setup for document tampering detection</h6>
	  </div>

	  	<h5 class="center brown-text">Text Detection</h5>
		<p>
			First step is to detect the text in the image. For this I use <a href="http://arxiv.org/abs/1609.03605">ctpn-textdetector</a> to extract text from the documents.  To extract characters from the text, we use connectedcomponents. 
		</p>

		<h5 class="center brown-text">Creating Patches</h5>
		<p>
			The input to the model are 64x64 patches. To extract patches from the image.  We use the bounding boxes previously extracted . Then 64x64 patches are created by extracting the region around the text. This becomes the input to our model.
		</p>

		<h5 class="center brown-text">Class Imbalance</h5>
		<p>
			As a document generally contains large amount of text out of which only a few regions are tampered. The data becomes highly imbalanced. In the find-it dataset we noticed that on average only 3.0% of patches extracted from a document were tampered.
		</p>

		<h3 class="center brown-text" id="experiment">Experiments</h3>		
		<div class="row">
			<h4 class="center black-test" > Changing Sampling Ratio </h4>
			<p> As we have very less tampered patches compared to non tampered patches.  The first experiment is to see the effect of sampling rate of different classes. Accordingly changing  the loss function. Hence after creating patches based as mentioned above. I tried 3 different sampling rate. </p>

			<table>
		        <thead>
		          <tr>
		              <th>Sampling Ratio( b/w tampered & non-tampered class )</th>
		              <th> Validation Patch Accuracy </th>
		              <th> Validation F1-score </th>
		          </tr>
		        </thead>
		        <tbody>
		          <tr>
		            <td>1-1</td>
		            <td>0.657</td>
		            <td>0.603</td>
		          </tr>
		          <tr>
		            <td>1-5</td>
		            <td>0.680</td>
		            <td>0.624</td>
		          </tr>
  		          <tr>
		            <td>1-10</td>
		            <td>0.664</td>
		            <td>0.608</td>
		          </tr>
		        </tbody>
	    	</table>
		</div>


		<div class="row">
			<h4 class="center black-test" > Amount of data augmentation </h4>

			As tampered images are so low.  It makes it very hard to train a CNN architecture with such data. As shown by Yashas [cite] data augmentation improves the performance of deep learning models on image tampering classifiation. 



		  <div class="row">
			<h5 class="center brown-text" style="margin: 0em">Synthetic Data creation</h5>
			<p>  We create synthetic tampered images using 3 types of tampering.</p>
			<ul>
			  <li >1. Copy-paste or CPI </li>
			  <li >2. Splicing or CPO </li>
			  <li >3. In-painting or CUT </li>
			</ul>

		  <ul class="collapsible">
			<li>
			  <div class="collapsible-header">Examples of augmented images created. </div>
			  <div style="width: 100%;align-items: flex-start" class="collapsible-body">
			  	<div class="row">
						<p class="center">Copy-Paste</p>
						<img style="width: 110%" src="./images/report_images/Figure_1-1.png" > 

						<p class="center">Splicing</p>
						<img style="width: 100%" src="./images/report_images/Figure_1-3.png" > 
					  
						<p class="center">Inpainting</p>
						<img style="width: 100%" src="./images/report_images/Figure_1-2.png" > 
				</div>
			  	</div>
				</li>
		  	</ul>
		  	<p> Find-it dataset provides 470 pristine images for classification. To create tampered images we first use a <a href="http://arxiv.org/abs/1609.03605">ctpn-textdetector [3]</a> to extract text from the documents.  To extract characters from the text, we use connectedcomponents.  Finally different types of tampering are created by replacing the text region.Thus, we create total of 6000 synthetic images. </p>
		  </div>

	
		</div>
		</div>

		<h5 class="center brown-text">Domain Adapatation</h5>
			<img class="center" style="width: 70% " src="./images/report_images/architecture.png" > 
		<p>
			As seen in the figure below during training source model and target model are trained simultaneously. The weights of the CNN block between them are shared.  Next they are passed through 2 fully connected layers, to get a 256-dim representation for source and target images. For domain adapation Yashas defines an MMD loss between the 2 representations. The MMD loss defined as:
		</p>
		<img class="center" style="width: 70%; padding-left: 30%" src="./images/report_images/mmd_loss.png" > 
		<p>where xs and xt are features for the source and target images respectively. Φ(xs) and Φ(xt) are the calculated by passing the representation through Gaussian kernel</p>

		<table>
	        <thead>
	          <tr>
	              <th> Number of synthetic patches </th>
		              <th> Source Training F1-score </th>
	              <th> Source Validation F1-score </th>

	              <th> Target Training data F1-score </th>
	              <th> Target Validation F1-score </th>
	          </tr>
	        </thead>
	        <tbody>
	          <tr>
	            <td> No synthetic data </td>
	            <td> - </td>
	            <td> - </td>
	            <td>0.680</td>
	            <td><b>0.624</b></td>
	          </tr>
	          <tr>
	            <td> 2000 Patches </td>
	            <td>0.968</td>
	            <td>0.951</td>
	            <td>0.962</td>
	            <td>0.734</td>
	          </tr>
		          <tr>
	            <td> 6000 </td>
	            <td>0.980</td>
	            <td>0.971</td>
	            <td>0.973</td>
	            <td><b>0.782</b></td>
	          </tr>
	        </tbody>
    	</table>




	  	<div class="row">
	  		<h6 class="center black-test" > Loss & Accuracy plots vs epochs</h6>
		  <ul class="collapsible">
			<li>
			  <div class="collapsible-header">Training & Validation loss with and without using augmented data</div>
			  <div class="collapsible-body">
				<div class="row">
					<img style="width: 100%" src="./images/report_images/loss_plot.png" > 
				</div>
			  </div>
			</li>
			<li>
			  <div class="collapsible-header"> Training and Validation accuracy with and without using augmented data</div>
			  <div style="width: 100%;align-items: flex-start" class="collapsible-body">
			  	<p>Validation accuracy on the find-it images is shown by pink color.</p>
				<img style="width: 100%" src="./images/report_images/acc_plot.png" > 
			  </div>
			</li>

		  </ul>
		</div>





		<div class="row">
			<h5 class="center black-test" > Changing CNN architecture </h5>
			<p>
				We futher tested whether changing the architecture would improve the performance. From the table below we can infer that resnet type model are performing slightly better than VGG setup used by Yashas
			</p>


			<table>
		        <thead>
		          <tr>
		              <th>Number of Convolution operations </th>
  		              <th> Source Training F1-score </th>
		              <th> Source Validation F1-score </th>

		              <th> Target Training data F1-score </th>
		              <th> Target Validation F1-score </th>
		          </tr>
		        </thead>
		        <tbody>
		          <tr>
		            <td> 3 block </td>
		            <td>0.902</td>
		            <td>0.898</td>
		            <td>0.87</td>
		            <td><b>0.702</b></td>
		          </tr>
		          <tr>
		            <td>5 blocks (Yashas Setup)</td>
		            <td>0.973</td>
		            <td>0.961</td>
		            <td>0.953</td>
		            <td><b>0.762</b></td>
		          </tr>
  		          <tr>
		            <td>18 block (Resnet Setup) </td>
		            <td>0.980</td>
		            <td>0.971</td>
		            <td>0.973</td>
		            <td><b>0.782</b></td>
		          </tr>
		        </tbody>
	    	</table>

	    	We notice a marginal improvement by changing the architecture.
		</div>




		<h5 class="center black-test" > Testing </h5>
		<p>
			 For testing we have taken the model with the highest average IOU over the validation data. We compare it with the setup used by Verdolivia. They use a 3 techniques (a) CMFD for CPI, (b) Splicebuster for CPO and (c) Jpeg Artifacts. I have taken the union of all 3 masks and compared it with the ground truth to compare with their results. 

		</p>
		<table>
	        <thead>
	          <tr>
	              <th>Average IOU on test data</th>
	          </tr>
	        </thead>
	        <tbody>
	          <tr>
	            <td>Without Augmentation</td>
	            <td>0.00138</td>
	          </tr>
	          <tr>
	            <td>With Augmentation</td>
	            <td>0.00178</td>
	          </tr>

              <tr>
	            <td><a href="https://shubhmaheshwari.github.io/Image_Tampering/#CMFD_method"> CMFD + Noiseprint + Jpeg Artifacts </a>  </td>
	            <td>0.516</td>
	          </tr>
	        </tbody>
	    </table>

		<h5 class="center black-test" > Qualtitative Results on Test set </h5>
		<ul class="collapsible">
			<li>
			  <div class="collapsible-header"> Our Setup</div>
			  <div class="row collapsible-body">
					<img style="width: 100%" src="./images/report_images/testbetter1.png" > 
					<img style="width: 100%" src="./images/report_images/testbetter2.png" > 
			  </div>
			</li>
			<li>
			  <div class="collapsible-header"> <a href="https://shubhmaheshwari.github.io/Image_Tampering/#CMFD_method"> CMFD + Noiseprint + Jpeg Artifacts </a> </div>
			  <div class="row collapsible-body">
					<img style="width: 100%" src="./images/report_images/prev1.png" > 
					<img style="width: 100%" src="./images/report_images/prev2.png" > 
					<img style="width: 100%" src="./images/report_images/prev3.png" > 
			  </div>
			</li>
		</ul>
		<p> For more results go to this <a href="./yashas_test_results.html">link</a> </p>

<!-- 		<h3 class="center brown-text" id="conclusion">Conclusion</h3>
		<ul>
			<li >1. Even after using augmented data the results were very poor on the find-it dataset</li>
			<li >2. We tried undersampling pristine patches but the model still seems to be overfitting on the training data.</li>
		</ul>
 -->		 
	  </div>
	</div>
  </div>  


<!--   <div class="container" id="current_experiment">
	<div class="section"> -->

	  <!--   Icon Section   -->
<!-- 	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Current Plan</h3> -->
			
<!-- 			<ul>
				<li >1. We are planning to change the objective from patch classification to semantic segmentation. </li>
				<li> 2. Also we are planning to use noiseprint features</li>
			</ul>


			<img style="width: 100%" src="./images/unet.png" > 
			<p><b>Clues why semantic segmentation would give better</b></p>
			<ul>
				<li>
					1. This should help in finding splicing artifacts as the spliced patch should have different PRNU features than the rest of the image.  
				</li>
				<li>
					2. This might help in copy-move based forgery if the model is able to find similiar patches. 
				</li>
				<li>	
					3. This should help in inpainting if the model is able to understand the pattern generated using inpainting.
				</li>
			</ul> -->

		<!-- Noise print -->            
<!-- 		<div class="row">
		  <b class="center black-text">Noiseprint Features<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8713484">[2]</a></b>
		  <p class="center light" style="margin: 0em"> The tampered region has noise inconsistencies in the tampered region. A CNN based network is used to extract noise from images whose architecture is shown below.
		  </p>
		  <img style="display: block;margin-left: auto;margin-right: auto;width: 50%;" src="./images/noiseprint.png" > <p class="center">The network architecture</p>
		  <img style="display: block;margin-left: auto;margin-right: auto;width: 50%;" src="./images/noiseprint-example.png" > <p class="center">Example of noise inconsistencies</p>
		  This network is used to detect forgeries in an image, as tampered regions will have inconsistencies in noise as shown below.
		</div>
	
	  </div>
	</div>
  </div>
 -->

<!--   <div class="container" id="improvements">
	<div class="section"> -->

	  <!--   Icon Section   -->
<!-- 	  <div class="row" style="font-size: 19px">
		<h3 class="center brown-text">Results on classification</h3>
		<p> </p>
	  </div> 
	</div>
  </div>
  </div>
 -->

  <footer class="page-footer teal">
	<div class="container">
	  <div class="row">
		<div class="col l6 s12">
		  <h5 class="white-text">Related Works</h5>
		  <ul>
			<li><a class="white-text" href="https://arxiv.org/pdf/1811.09729v3.pdf">[1] Generate, Segment and Refine: Towards Generic Manipulation Segmentation</a></li>
			<li><a class="white-text" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8713484">[2] Noiseprint: A CNN-Based Camera Model Fingerprint</a></li>
			<li><a class="white-text" href="https://minyoungg.github.io/selfconsistency/">[3] Z. Tian, W. Huang, T. He, P. He, Y. Qiao, Detecting text in natural image with connectionist text proposal network (2016)</a></li>
			<li><a class="white-text" href="http://dx.doi.org/10.1109/ICPR.2018.8545428">[4] C. Artaud, N. Sidre, A. Doucet, J. Ogier, V. P. D. Yooz, Find it! fraud detection contest report, in: 2018 24th InternationalConference on Pattern Recognition (ICPR)</a></li>
		  </ul>
		</div>
		<div class="col l6 s12">
		  <ul>
		  	<br>
		  	<br>
			<li><a class="white-text" href="https://arxiv.org/pdf/1811.09729v3.pdf">[5] Generate, Segment and Refine: Towards Generic Manipulation Segmentation</a></li>
			<li><a class="white-text" href="https://arxiv.org/pdf/1906.11663.pdf">[6] SpliceRadar: A Learned Method For Blind Image Forensics</a></li>
			<li><a class="white-text" href="https://minyoungg.github.io/selfconsistency/">[7] Fighting Fake News: Image Splice Detection via Learned Self-Consistency</a></li>
		  </ul>
		</div>
		</div>
	</div>
	<div class="footer-copyright">
	  <div class="container">
	  Made by <a class="brown-text text-lighten-3" href="#">Shubh </a>
	  </div>
	</div>
  </footer>


  <!--  Scripts-->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="js/materialize.js"></script>
  <script src="js/init.js"></script>

  </body>
</html>
